{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five rows of train data frame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>City</th>\n",
       "      <th>Zip or postal code</th>\n",
       "      <th>Price</th>\n",
       "      <th>Beds</th>\n",
       "      <th>Baths</th>\n",
       "      <th>Location</th>\n",
       "      <th>Square feet</th>\n",
       "      <th>Lot size</th>\n",
       "      <th>Year built</th>\n",
       "      <th>Hoa/month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1048</td>\n",
       "      <td>Olathe</td>\n",
       "      <td>66062.0</td>\n",
       "      <td>350000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Quail Park</td>\n",
       "      <td>1876.0</td>\n",
       "      <td>11761.0</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>406</td>\n",
       "      <td>Mission</td>\n",
       "      <td>66202.0</td>\n",
       "      <td>297000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>DILLE HEIGHTS</td>\n",
       "      <td>1559.0</td>\n",
       "      <td>15001.0</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1027</td>\n",
       "      <td>Olathe</td>\n",
       "      <td>66062.0</td>\n",
       "      <td>279950.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Arrowhead East</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>7405.0</td>\n",
       "      <td>1982.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>486</td>\n",
       "      <td>Overland Park</td>\n",
       "      <td>66204.0</td>\n",
       "      <td>299000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rayven Plains</td>\n",
       "      <td>1404.0</td>\n",
       "      <td>11353.0</td>\n",
       "      <td>1950.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1485</td>\n",
       "      <td>Lenexa</td>\n",
       "      <td>66215.0</td>\n",
       "      <td>531000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Oak Valley</td>\n",
       "      <td>3542.0</td>\n",
       "      <td>12553.0</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0           City  Zip or postal code     Price  Beds  Baths  \\\n",
       "0        1048         Olathe             66062.0  350000.0   3.0    3.0   \n",
       "1         406        Mission             66202.0  297000.0   2.0    2.5   \n",
       "2        1027         Olathe             66062.0  279950.0   3.0    2.0   \n",
       "3         486  Overland Park             66204.0  299000.0   3.0    1.0   \n",
       "4        1485         Lenexa             66215.0  531000.0   4.0    3.5   \n",
       "\n",
       "         Location  Square feet  Lot size  Year built  Hoa/month  \n",
       "0      Quail Park       1876.0   11761.0      1995.0       13.0  \n",
       "1   DILLE HEIGHTS       1559.0   15001.0      1990.0        0.0  \n",
       "2  Arrowhead East       1344.0    7405.0      1982.0        0.0  \n",
       "3   Rayven Plains       1404.0   11353.0      1950.0        0.0  \n",
       "4      Oak Valley       3542.0   12553.0      1994.0       20.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "First five rows of test data frame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>City</th>\n",
       "      <th>Zip or postal code</th>\n",
       "      <th>Price</th>\n",
       "      <th>Beds</th>\n",
       "      <th>Baths</th>\n",
       "      <th>Location</th>\n",
       "      <th>Square feet</th>\n",
       "      <th>Lot size</th>\n",
       "      <th>Year built</th>\n",
       "      <th>Hoa/month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>917</td>\n",
       "      <td>Olathe</td>\n",
       "      <td>66061.0</td>\n",
       "      <td>749500.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Cedar Creek - Valley Ridge</td>\n",
       "      <td>2884.0</td>\n",
       "      <td>9587.0</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>156.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>134</td>\n",
       "      <td>Overland Park</td>\n",
       "      <td>66224.0</td>\n",
       "      <td>939000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>WatersEdge</td>\n",
       "      <td>3880.0</td>\n",
       "      <td>11944.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>131.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>390</td>\n",
       "      <td>Shawnee</td>\n",
       "      <td>66203.0</td>\n",
       "      <td>265000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Tomahawk Hills</td>\n",
       "      <td>1136.0</td>\n",
       "      <td>9877.0</td>\n",
       "      <td>1962.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>790</td>\n",
       "      <td>Olathe</td>\n",
       "      <td>66061.0</td>\n",
       "      <td>308000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Rose Addition</td>\n",
       "      <td>1808.0</td>\n",
       "      <td>12207.0</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1498</td>\n",
       "      <td>Lenexa</td>\n",
       "      <td>66215.0</td>\n",
       "      <td>345000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>Century Estates</td>\n",
       "      <td>1766.0</td>\n",
       "      <td>14249.0</td>\n",
       "      <td>1972.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0           City  Zip or postal code     Price  Beds  Baths  \\\n",
       "0         917         Olathe             66061.0  749500.0   6.0    4.0   \n",
       "1         134  Overland Park             66224.0  939000.0   4.0    4.5   \n",
       "2         390        Shawnee             66203.0  265000.0   3.0    2.0   \n",
       "3         790         Olathe             66061.0  308000.0   3.0    2.0   \n",
       "4        1498         Lenexa             66215.0  345000.0   3.0    2.5   \n",
       "\n",
       "                     Location  Square feet  Lot size  Year built  Hoa/month  \n",
       "0  Cedar Creek - Valley Ridge       2884.0    9587.0      2023.0      156.0  \n",
       "1                  WatersEdge       3880.0   11944.0      2016.0      131.0  \n",
       "2              Tomahawk Hills       1136.0    9877.0      1962.0        0.0  \n",
       "3               Rose Addition       1808.0   12207.0      1964.0        0.0  \n",
       "4             Century Estates       1766.0   14249.0      1972.0        0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "First five rows of train data frame after removing unnecesary column:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Zip or postal code</th>\n",
       "      <th>Price</th>\n",
       "      <th>Beds</th>\n",
       "      <th>Baths</th>\n",
       "      <th>Location</th>\n",
       "      <th>Square feet</th>\n",
       "      <th>Lot size</th>\n",
       "      <th>Year built</th>\n",
       "      <th>Hoa/month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Olathe</td>\n",
       "      <td>66062.0</td>\n",
       "      <td>350000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Quail Park</td>\n",
       "      <td>1876.0</td>\n",
       "      <td>11761.0</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mission</td>\n",
       "      <td>66202.0</td>\n",
       "      <td>297000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>DILLE HEIGHTS</td>\n",
       "      <td>1559.0</td>\n",
       "      <td>15001.0</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Olathe</td>\n",
       "      <td>66062.0</td>\n",
       "      <td>279950.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Arrowhead East</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>7405.0</td>\n",
       "      <td>1982.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Overland Park</td>\n",
       "      <td>66204.0</td>\n",
       "      <td>299000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rayven Plains</td>\n",
       "      <td>1404.0</td>\n",
       "      <td>11353.0</td>\n",
       "      <td>1950.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lenexa</td>\n",
       "      <td>66215.0</td>\n",
       "      <td>531000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Oak Valley</td>\n",
       "      <td>3542.0</td>\n",
       "      <td>12553.0</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            City  Zip or postal code     Price  Beds  Baths        Location  \\\n",
       "0         Olathe             66062.0  350000.0   3.0    3.0      Quail Park   \n",
       "1        Mission             66202.0  297000.0   2.0    2.5   DILLE HEIGHTS   \n",
       "2         Olathe             66062.0  279950.0   3.0    2.0  Arrowhead East   \n",
       "3  Overland Park             66204.0  299000.0   3.0    1.0   Rayven Plains   \n",
       "4         Lenexa             66215.0  531000.0   4.0    3.5      Oak Valley   \n",
       "\n",
       "   Square feet  Lot size  Year built  Hoa/month  \n",
       "0       1876.0   11761.0      1995.0       13.0  \n",
       "1       1559.0   15001.0      1990.0        0.0  \n",
       "2       1344.0    7405.0      1982.0        0.0  \n",
       "3       1404.0   11353.0      1950.0        0.0  \n",
       "4       3542.0   12553.0      1994.0       20.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "First five rows of test data frame after removing unnecesary column:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Zip or postal code</th>\n",
       "      <th>Price</th>\n",
       "      <th>Beds</th>\n",
       "      <th>Baths</th>\n",
       "      <th>Location</th>\n",
       "      <th>Square feet</th>\n",
       "      <th>Lot size</th>\n",
       "      <th>Year built</th>\n",
       "      <th>Hoa/month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Olathe</td>\n",
       "      <td>66061.0</td>\n",
       "      <td>749500.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Cedar Creek - Valley Ridge</td>\n",
       "      <td>2884.0</td>\n",
       "      <td>9587.0</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>156.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Overland Park</td>\n",
       "      <td>66224.0</td>\n",
       "      <td>939000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>WatersEdge</td>\n",
       "      <td>3880.0</td>\n",
       "      <td>11944.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>131.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shawnee</td>\n",
       "      <td>66203.0</td>\n",
       "      <td>265000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Tomahawk Hills</td>\n",
       "      <td>1136.0</td>\n",
       "      <td>9877.0</td>\n",
       "      <td>1962.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Olathe</td>\n",
       "      <td>66061.0</td>\n",
       "      <td>308000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Rose Addition</td>\n",
       "      <td>1808.0</td>\n",
       "      <td>12207.0</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lenexa</td>\n",
       "      <td>66215.0</td>\n",
       "      <td>345000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>Century Estates</td>\n",
       "      <td>1766.0</td>\n",
       "      <td>14249.0</td>\n",
       "      <td>1972.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            City  Zip or postal code     Price  Beds  Baths  \\\n",
       "0         Olathe             66061.0  749500.0   6.0    4.0   \n",
       "1  Overland Park             66224.0  939000.0   4.0    4.5   \n",
       "2        Shawnee             66203.0  265000.0   3.0    2.0   \n",
       "3         Olathe             66061.0  308000.0   3.0    2.0   \n",
       "4         Lenexa             66215.0  345000.0   3.0    2.5   \n",
       "\n",
       "                     Location  Square feet  Lot size  Year built  Hoa/month  \n",
       "0  Cedar Creek - Valley Ridge       2884.0    9587.0      2023.0      156.0  \n",
       "1                  WatersEdge       3880.0   11944.0      2016.0      131.0  \n",
       "2              Tomahawk Hills       1136.0    9877.0      1962.0        0.0  \n",
       "3               Rose Addition       1808.0   12207.0      1964.0        0.0  \n",
       "4             Century Estates       1766.0   14249.0      1972.0        0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The below lines of code import the pandas library and the display function.\n",
    "import pandas as pd \n",
    "from IPython.display import display\n",
    "\n",
    "# The below lines of code read in the train_df and test_df CSV files.\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# The below lines of code display the first five rows of the train data frame.\n",
    "print('First five rows of train data frame:')\n",
    "display(train.head())\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "# The below lines of code display the first five rows of the test data frame.\n",
    "print('First five rows of test data frame:')\n",
    "display(test.head())\n",
    "\n",
    "# The below lines of code delete the previous index column in both the train and test data frames. \n",
    "del train['Unnamed: 0']\n",
    "del test['Unnamed: 0']\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "# The below lines of code display the first five rows of the train data frame.\n",
    "print('First five rows of train data frame after removing unnecesary column:')\n",
    "display(train.head())\n",
    "\n",
    "print()\n",
    "print()\n",
    "# The below lines of code display the first five rows of the test data frame.\n",
    "print('First five rows of test data frame after removing unnecesary column:')\n",
    "display(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing transformation functions and libraries\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "\n",
    "\n",
    "# The below lines of code assign the Price column from the train and test data frames as the target variables.\n",
    "train_target = train['Price']\n",
    "test_target = test['Price']\n",
    "\n",
    "# The below lines of code delete the Price and Location column from the train and test data frames. \n",
    "del train['Price']\n",
    "del test['Price']\n",
    "del train['Location']\n",
    "del test['Location']\n",
    "\n",
    "# Creating list to hold names of numerical and categorical features\n",
    "cat_feat = ['City','Zip or postal code']\n",
    "num_feat = ['Beds','Baths', 'Square feet', 'Lot size', 'Year built', 'Hoa/month']\n",
    "\n",
    "# Creating pipe1, which will logarithmically transform the features and then scale them.\n",
    "pipe1 = Pipeline([\n",
    "    ('log', FunctionTransformer(func=np.log1p)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ])\n",
    "# Creating a column transform that will send the numerical features to pipe1 and then one-hot encode the categorical features (while dropping the first column).\n",
    "big_pipeline = ColumnTransformer([\n",
    "    ('num', pipe1, num_feat),\n",
    "    ('cat', OneHotEncoder(drop='first'), cat_feat)\n",
    "])\n",
    "\n",
    "# Fitting big pipeline with train data, transforming train data using big_pipeline, and transforming testing data using big_pipeline.\n",
    "train_og = big_pipeline.fit_transform(train)\n",
    "test_og = big_pipeline.transform(test)\n",
    "\n",
    "# Creating pipe2, which will yeo-johnson transform the features and then scale them.\n",
    "pipe2 = Pipeline([\n",
    "    ('yeo-john', PowerTransformer(method='yeo-johnson', standardize=False)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ])\n",
    "# Creating a column transformer that will send the numerical features to pipe2 and then one-hot encode the categorical features (while dropping the first column and not outputting as a sparse matrix).\n",
    "big_pipeline2 = ColumnTransformer([\n",
    "    ('num', pipe2, num_feat),\n",
    "    ('cat', OneHotEncoder(drop='first', sparse_output=False), cat_feat)\n",
    "])\n",
    "\n",
    "# Fitting big_pipeline2 with train data, transform train data using big_pipeline2, and transforming test data using big_pipeline2.\n",
    "train_2 = big_pipeline2.fit_transform(train)\n",
    "test_2 = big_pipeline2.transform(test)\n",
    "\n",
    "\n",
    "# The below line of code creates a logarithmically transformed target variable.\n",
    "train_target_log = np.log(train_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in regressor model, scoring function, and search function to determine best combination of parameters.\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating first gradient boosting regressor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE on training data with original pipeline: 49173.64\n",
      "MAE on testing data with original pipeline 69912.08\n",
      "\n",
      "MAE on training data with second pipeline: 49173.64\n",
      "MAE on testing data with second pipeline 69776.05\n"
     ]
    }
   ],
   "source": [
    "# Fitting a gradient boosting regressor with data trained by big_pipeline1, and testing said model on test data transformed by big_pipeline1.\n",
    "gbr = GradientBoostingRegressor(random_state=22)\n",
    "gbr.fit(train_og, train_target_log)\n",
    "pred_target_log = gbr.predict(train_og)\n",
    "pred_target = np.exp(pred_target_log)\n",
    "mae = np.round(mean_absolute_error(train_target, pred_target),2)\n",
    "print('MAE on training data with original pipeline:', mae)\n",
    "pred_target_log = gbr.predict(test_og)\n",
    "pred_target = np.exp(pred_target_log)\n",
    "mae = np.round(mean_absolute_error(test_target, pred_target),2)\n",
    "print('MAE on testing data with original pipeline', mae)\n",
    "\n",
    "\n",
    "print()\n",
    "# Fitting a gradient boosting regressor with data trained by big_pipeline2, and testing said model on test data transformed by big_pipeline2.\n",
    "gbr = GradientBoostingRegressor(random_state=22)\n",
    "gbr.fit(train_2, train_target_log)\n",
    "pred_target_log = gbr.predict(train_2)\n",
    "pred_target = np.exp(pred_target_log)\n",
    "mae = np.round(mean_absolute_error(train_target, pred_target),2)\n",
    "print('MAE on training data with second pipeline:', mae)\n",
    "pred_target_log = gbr.predict(test_2)\n",
    "pred_target = np.exp(pred_target_log)\n",
    "mae = np.round(mean_absolute_error(test_target, pred_target),2)\n",
    "print('MAE on testing data with second pipeline', mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The Gradient Boosting Regressor model fit with predictors from the original pipeline has a training MAE of 49173.64 and a testing MAE of 69912.64. Considering that the testing MAE is fairly low, I am surprised that the difference between the training and testing MAE is only around 20,000. In previous notebooks, it seems like whenever this difference decreases, the testing MAE would skyrocket.\n",
    "\n",
    "The second model contains the same training MAE but slightly improved the testing MAE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.9,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'criterion': 'friedman_mse',\n",
       " 'init': None,\n",
       " 'learning_rate': 0.1,\n",
       " 'loss': 'squared_error',\n",
       " 'max_depth': 3,\n",
       " 'max_features': None,\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_iter_no_change': None,\n",
       " 'random_state': 22,\n",
       " 'subsample': 1.0,\n",
       " 'tol': 0.0001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating second Gradient Boosting Regressor models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is that hopefully by reducing the number of estimators, the model won't have as many chances to get better at knowing the train data as well, and will be able to generalize to unseen data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE on training data with original pipeline: 52499.0\n",
      "MAE on testing data with original pipeline 71541.0\n",
      "\n",
      "MAE on training data with second pipeline: 52499.0\n",
      "MAE on testing data with second pipeline 71450.0\n"
     ]
    }
   ],
   "source": [
    "gbr = GradientBoostingRegressor(random_state=22, n_estimators=75)\n",
    "# Fitting the gbr with train data transformed by big_pipeline1 and testing gbr with test data transformed by big_pipeline1.\n",
    "gbr.fit(train_og, train_target_log)\n",
    "pred_target_log = gbr.predict(train_og)\n",
    "pred_target = np.exp(pred_target_log)\n",
    "mae = np.round(mean_absolute_error(train_target, pred_target))\n",
    "print('MAE on training data with original pipeline:', mae)\n",
    "pred_target_log = gbr.predict(test_og)\n",
    "pred_target = np.exp(pred_target_log)\n",
    "mae = np.round(mean_absolute_error(test_target, pred_target))\n",
    "print('MAE on testing data with original pipeline', mae)\n",
    "\n",
    "print()\n",
    "# Fitting the gbr with train data transformed by big_pipeline2 and testing gbr with test data transformed by big_pipeline2.\n",
    "gbr.fit(train_2, train_target_log)\n",
    "pred_target_log = gbr.predict(train_2)\n",
    "pred_target = np.exp(pred_target_log)\n",
    "mae = np.round(mean_absolute_error(train_target, pred_target))\n",
    "print('MAE on training data with second pipeline:', mae)\n",
    "pred_target_log = gbr.predict(test_2)\n",
    "pred_target = np.exp(pred_target_log)\n",
    "mae = np.round(mean_absolute_error(test_target, pred_target))\n",
    "print('MAE on testing data with second pipeline', mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both train and test data MAE increased, indicating the reducing the number of estimators didn't have the impact I was hoping for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempting to use Random Search CV to find a model that generalizes better to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-27 {color: black;background-color: white;}#sk-container-id-27 pre{padding: 0;}#sk-container-id-27 div.sk-toggleable {background-color: white;}#sk-container-id-27 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-27 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-27 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-27 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-27 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-27 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-27 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-27 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-27 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-27 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-27 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-27 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-27 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-27 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-27 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-27 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-27 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-27 div.sk-item {position: relative;z-index: 1;}#sk-container-id-27 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-27 div.sk-item::before, #sk-container-id-27 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-27 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-27 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-27 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-27 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-27 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-27 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-27 div.sk-label-container {text-align: center;}#sk-container-id-27 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-27 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-27\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=4, estimator=GradientBoostingRegressor(random_state=42),\n",
       "                   param_distributions={&#x27;learning_rate&#x27;: array([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 , 0.11,\n",
       "       0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21, 0.22,\n",
       "       0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32, 0.33,\n",
       "       0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43, 0.44,\n",
       "       0.45, 0.46, 0.4...\n",
       "       0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43, 0.44,\n",
       "       0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54, 0.55,\n",
       "       0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65, 0.66,\n",
       "       0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77,\n",
       "       0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88,\n",
       "       0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99])},\n",
       "                   random_state=22, return_train_score=True,\n",
       "                   scoring=&#x27;neg_mean_absolute_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-57\" type=\"checkbox\" ><label for=\"sk-estimator-id-57\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=4, estimator=GradientBoostingRegressor(random_state=42),\n",
       "                   param_distributions={&#x27;learning_rate&#x27;: array([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 , 0.11,\n",
       "       0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21, 0.22,\n",
       "       0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32, 0.33,\n",
       "       0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43, 0.44,\n",
       "       0.45, 0.46, 0.4...\n",
       "       0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43, 0.44,\n",
       "       0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54, 0.55,\n",
       "       0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65, 0.66,\n",
       "       0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77,\n",
       "       0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88,\n",
       "       0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99])},\n",
       "                   random_state=22, return_train_score=True,\n",
       "                   scoring=&#x27;neg_mean_absolute_error&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-58\" type=\"checkbox\" ><label for=\"sk-estimator-id-58\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-59\" type=\"checkbox\" ><label for=\"sk-estimator-id-59\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=4, estimator=GradientBoostingRegressor(random_state=42),\n",
       "                   param_distributions={'learning_rate': array([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 , 0.11,\n",
       "       0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21, 0.22,\n",
       "       0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32, 0.33,\n",
       "       0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43, 0.44,\n",
       "       0.45, 0.46, 0.4...\n",
       "       0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43, 0.44,\n",
       "       0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54, 0.55,\n",
       "       0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65, 0.66,\n",
       "       0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77,\n",
       "       0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88,\n",
       "       0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99])},\n",
       "                   random_state=22, return_train_score=True,\n",
       "                   scoring='neg_mean_absolute_error')"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following lines of code create a dictionary with different hyperparameter values.\n",
    "param_grid = {\n",
    "    'n_estimators': np.arange(1,200,5),\n",
    "    'max_depth': np.arange(5,100,5),\n",
    "    'min_samples_split': np.arange(2,20,1),\n",
    "    'learning_rate': np.arange(.01,1,.01),\n",
    "    'subsample': np.arange(.01,1,.01),\n",
    "}\n",
    "\n",
    "\n",
    "# The below line of code specifies an initial Gradient Boosting Regressor.\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# The following initializes a RandomizedSearchCV model with the Gradient Boosting Regressor, using param_grid for hyperparameters, neg_mean_absolute_error as the scoring metric, 4-fold cross-validation, and a random state for reproducibility.\n",
    "rscv = RandomizedSearchCV(gbr, param_distributions=param_grid, scoring='neg_mean_absolute_error', cv=4, return_train_score=True, random_state=22)\n",
    "\n",
    "# Fitting the randomizedsearchCV with data transformed by big_pipeline2.\n",
    "rscv.fit(train_2, train_target_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE on training data with original pipeline: 43019.0\n",
      "MAE on testing data with original pipeline 73003.0\n",
      "\n",
      "MAE on training data with second pipeline: 44263.0\n",
      "MAE on testing data with second pipeline 72734.0\n"
     ]
    }
   ],
   "source": [
    "rs = rscv.best_estimator_\n",
    "# Fitting the best estimator from the randomized search function with train data transformed by big_pipeline1 and testing said estimator on test data transformed by big_pipeline1.\n",
    "rs.fit(train_og, train_target_log)\n",
    "pred_target_log = rs.predict(train_og)\n",
    "pred_target = np.exp(pred_target_log)\n",
    "mae = np.round(mean_absolute_error(train_target, pred_target))\n",
    "print('MAE on training data with original pipeline:', mae)\n",
    "pred_target_log = rs.predict(test_og)\n",
    "pred_target = np.exp(pred_target_log)\n",
    "mae = np.round(mean_absolute_error(test_target, pred_target))\n",
    "print('MAE on testing data with original pipeline', mae)\n",
    "\n",
    "print()\n",
    "# Fitting the best estimator from the randomized search function with train data transformed by big_pipeline2 and testing said estimator on test data transformed by big_pipeline2.\n",
    "rs.fit(train_2, train_target_log)\n",
    "pred_target_log = rs.predict(train_2)\n",
    "pred_target = np.exp(pred_target_log)\n",
    "mae = np.round(mean_absolute_error(train_target, pred_target))\n",
    "print('MAE on training data with second pipeline:', mae)\n",
    "pred_target_log = rs.predict(test_2)\n",
    "pred_target = np.exp(pred_target_log)\n",
    "mae = np.round(mean_absolute_error(test_target, pred_target))\n",
    "print('MAE on testing data with second pipeline', mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results, there are still clear signs of innaccuraies and overfitting. To hopefully address this, I plan to use the RandomizedSearchCV function with a custom score. Currently, I've been using either train_target (normal values) or train_target_log (logarithmically transformed value) as what I am feeding into the RandomizedSearchCV function as the target.\n",
    "\n",
    "Both these will lead to issues.\n",
    "\n",
    "1. When using train_target, the default scoring will work perfectly fine in terms of scale. However, I have been assuming that when I retrain the model, that the transformed target variables will allow the model to perform at its best. \n",
    "\n",
    "2. When using the train_target_log, the scoring is based on logarithmically scaled values, which doesn't align with how I plan to evaluate the models. Instead, I need scoring models that can take the logarithmically transformed true target values and transform them properly, and then transformed the predicted target values, and then I will be able to calculate a suitable mean absolute error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizing a custom scorer in the Randomized Search CV function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-28 {color: black;background-color: white;}#sk-container-id-28 pre{padding: 0;}#sk-container-id-28 div.sk-toggleable {background-color: white;}#sk-container-id-28 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-28 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-28 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-28 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-28 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-28 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-28 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-28 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-28 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-28 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-28 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-28 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-28 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-28 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-28 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-28 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-28 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-28 div.sk-item {position: relative;z-index: 1;}#sk-container-id-28 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-28 div.sk-item::before, #sk-container-id-28 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-28 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-28 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-28 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-28 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-28 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-28 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-28 div.sk-label-container {text-align: center;}#sk-container-id-28 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-28 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-28\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5, estimator=GradientBoostingRegressor(random_state=22),\n",
       "                   param_distributions={&#x27;learning_rate&#x27;: array([0.05, 0.08, 0.11, 0.14, 0.17, 0.2 , 0.23, 0.26, 0.29, 0.32]),\n",
       "                                        &#x27;max_depth&#x27;: array([ 1,  6, 11, 16, 21, 26, 31, 36, 41, 46, 51, 56, 61, 66, 71, 76, 81,\n",
       "       86, 91, 96]),\n",
       "                                        &#x27;min_samples_split&#x27;: array([ 2,  4,  6,  8, 10, 12, 14, 16, 18]),\n",
       "                                        &#x27;n_estimators&#x27;: array([  1,   6,  11,  16,  21,  26,  31,  36,  41,...\n",
       "       131, 136, 141, 146, 151, 156, 161, 166, 171, 176, 181, 186, 191,\n",
       "       196, 201, 206, 211, 216, 221, 226, 231, 236, 241, 246, 251, 256,\n",
       "       261, 266, 271, 276, 281, 286, 291, 296, 301, 306, 311, 316, 321,\n",
       "       326, 331, 336, 341, 346, 351, 356, 361, 366, 371, 376, 381, 386,\n",
       "       391, 396]),\n",
       "                                        &#x27;subsample&#x27;: array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])},\n",
       "                   random_state=22, return_train_score=True,\n",
       "                   scoring=make_scorer(custom_mae, greater_is_better=False))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-60\" type=\"checkbox\" ><label for=\"sk-estimator-id-60\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5, estimator=GradientBoostingRegressor(random_state=22),\n",
       "                   param_distributions={&#x27;learning_rate&#x27;: array([0.05, 0.08, 0.11, 0.14, 0.17, 0.2 , 0.23, 0.26, 0.29, 0.32]),\n",
       "                                        &#x27;max_depth&#x27;: array([ 1,  6, 11, 16, 21, 26, 31, 36, 41, 46, 51, 56, 61, 66, 71, 76, 81,\n",
       "       86, 91, 96]),\n",
       "                                        &#x27;min_samples_split&#x27;: array([ 2,  4,  6,  8, 10, 12, 14, 16, 18]),\n",
       "                                        &#x27;n_estimators&#x27;: array([  1,   6,  11,  16,  21,  26,  31,  36,  41,...\n",
       "       131, 136, 141, 146, 151, 156, 161, 166, 171, 176, 181, 186, 191,\n",
       "       196, 201, 206, 211, 216, 221, 226, 231, 236, 241, 246, 251, 256,\n",
       "       261, 266, 271, 276, 281, 286, 291, 296, 301, 306, 311, 316, 321,\n",
       "       326, 331, 336, 341, 346, 351, 356, 361, 366, 371, 376, 381, 386,\n",
       "       391, 396]),\n",
       "                                        &#x27;subsample&#x27;: array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])},\n",
       "                   random_state=22, return_train_score=True,\n",
       "                   scoring=make_scorer(custom_mae, greater_is_better=False))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-61\" type=\"checkbox\" ><label for=\"sk-estimator-id-61\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(random_state=22)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-62\" type=\"checkbox\" ><label for=\"sk-estimator-id-62\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(random_state=22)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=GradientBoostingRegressor(random_state=22),\n",
       "                   param_distributions={'learning_rate': array([0.05, 0.08, 0.11, 0.14, 0.17, 0.2 , 0.23, 0.26, 0.29, 0.32]),\n",
       "                                        'max_depth': array([ 1,  6, 11, 16, 21, 26, 31, 36, 41, 46, 51, 56, 61, 66, 71, 76, 81,\n",
       "       86, 91, 96]),\n",
       "                                        'min_samples_split': array([ 2,  4,  6,  8, 10, 12, 14, 16, 18]),\n",
       "                                        'n_estimators': array([  1,   6,  11,  16,  21,  26,  31,  36,  41,...\n",
       "       131, 136, 141, 146, 151, 156, 161, 166, 171, 176, 181, 186, 191,\n",
       "       196, 201, 206, 211, 216, 221, 226, 231, 236, 241, 246, 251, 256,\n",
       "       261, 266, 271, 276, 281, 286, 291, 296, 301, 306, 311, 316, 321,\n",
       "       326, 331, 336, 341, 346, 351, 356, 361, 366, 371, 376, 381, 386,\n",
       "       391, 396]),\n",
       "                                        'subsample': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])},\n",
       "                   random_state=22, return_train_score=True,\n",
       "                   scoring=make_scorer(custom_mae, greater_is_better=False))"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the make_scorer function\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Creating a dictionary for potential parameter options.\n",
    "param_grid = {\n",
    "    'n_estimators': np.arange(1,400,5),\n",
    "    'max_depth': np.arange(1,100,5),\n",
    "    'min_samples_split': np.arange(2,20,2),\n",
    "    'learning_rate': np.arange(.05,.35,.03),\n",
    "    'subsample': np.arange(.1,1,.1),\n",
    "}\n",
    "\n",
    "# The below code is property of Gemini. I have had issues before with creating custom scoring functions, so I apologize if anyone reading this notebook does't like that.\n",
    "def custom_mae(y_true, y_pred):\n",
    "    y_true_original = np.exp(y_true)\n",
    "    y_pred_original = np.exp(y_pred)\n",
    "    return mean_absolute_error(y_true_original, y_pred_original)\n",
    "custom_scorer = make_scorer(custom_mae, greater_is_better=False) \n",
    "\n",
    "\n",
    "# The below line of code specifies an initial Gradient Boosting Regressor.\n",
    "gbr = GradientBoostingRegressor(random_state=22)\n",
    "\n",
    "# The following initializes a RandomizedSearchCV model with the Gradient Boosting Regressor, using param_grid for hyperparameters, custom_score as the scoring metric, 5-fold cross-validation, and a random state for reproducibility.\n",
    "rscv = RandomizedSearchCV(gbr, param_distributions=param_grid, scoring=custom_scorer, cv=5, return_train_score=True, random_state=22)\n",
    "\n",
    "rscv.fit(train_2, train_target_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although not all the above code was me, I am still excited that a custom_scoring function was finally made that would work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-26 {color: black;background-color: white;}#sk-container-id-26 pre{padding: 0;}#sk-container-id-26 div.sk-toggleable {background-color: white;}#sk-container-id-26 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-26 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-26 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-26 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-26 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-26 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-26 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-26 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-26 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-26 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-26 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-26 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-26 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-26 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-26 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-26 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-26 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-26 div.sk-item {position: relative;z-index: 1;}#sk-container-id-26 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-26 div.sk-item::before, #sk-container-id-26 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-26 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-26 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-26 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-26 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-26 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-26 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-26 div.sk-label-container {text-align: center;}#sk-container-id-26 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-26 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-26\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(learning_rate=0.16999999999999998, max_depth=21,\n",
       "                          min_samples_split=14, n_estimators=321,\n",
       "                          random_state=22, subsample=0.5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-56\" type=\"checkbox\" checked><label for=\"sk-estimator-id-56\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=0.16999999999999998, max_depth=21,\n",
       "                          min_samples_split=14, n_estimators=321,\n",
       "                          random_state=22, subsample=0.5)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.16999999999999998, max_depth=21,\n",
       "                          min_samples_split=14, n_estimators=321,\n",
       "                          random_state=22, subsample=0.5)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rscv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the parameters, there does seem to be some clear changes between the default values and the model that was determined to result in the lowest MAE for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE on training data with original pipeline: 3225.0\n",
      "MAE on testing data with original pipeline 70720.0\n",
      "\n",
      "MAE on training data with second pipeline: 3241.0\n",
      "MAE on testing data with second pipeline 70038.0\n"
     ]
    }
   ],
   "source": [
    "model = rscv.best_estimator_\n",
    "# Fitting the best estimator with train data transformed by big_pipeline1 and testing said model on test data transformed by big_pipeline1.\n",
    "model.fit(train_og, train_target_log)\n",
    "pred_target_log = model.predict(train_og)\n",
    "pred_target = np.exp(pred_target_log)\n",
    "mae = np.round(mean_absolute_error(train_target, pred_target))\n",
    "print('MAE on training data with original pipeline:', mae)\n",
    "pred_target_log = model.predict(test_og)\n",
    "pred_target = np.exp(pred_target_log)\n",
    "mae = np.round(mean_absolute_error(test_target, pred_target))\n",
    "print('MAE on testing data with original pipeline', mae)\n",
    "\n",
    "print()\n",
    "# Fitting the best estimator with train data transformed by big_pipeline2 and testing said model on test data transformed by big_pipeline2.\n",
    "model.fit(train_2, train_target_log)\n",
    "pred_target_log = model.predict(train_2)\n",
    "pred_target = np.exp(pred_target_log)\n",
    "mae = np.round(mean_absolute_error(train_target, pred_target))\n",
    "print('MAE on training data with second pipeline:', mae)\n",
    "pred_target_log = model.predict(test_2)\n",
    "pred_target = np.exp(pred_target_log)\n",
    "mae = np.round(mean_absolute_error(test_target, pred_target))\n",
    "print('MAE on testing data with second pipeline', mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the training MAEs are around 3220, while testing MAEs are around 70000. This difference is perplexing because the RandomizedSearchCV function splits the data into five sections for cross-validation, which should allow the model to generalize well to unseen data by testing it on each held-out section. However, the model performs exceptionally well on the training data but fails to generalize to the testing data. \n",
    "\n",
    "One possible reason for this simply could be that the RandomizedSearchCV function was given too many possibilities in terms of parameter combinations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limiting the number of parameter options in the Randomized Search CV function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE on training data with second pipeline: 58724.0\n",
      "MAE on testing data with second pipeline 73305.0\n"
     ]
    }
   ],
   "source": [
    "# The following lines of code create a dictionary with different hyperparameter values.\n",
    "param_grid = {\n",
    "    'n_estimators': np.arange(1,200,1),\n",
    "    'max_depth': np.arange(1,10,1),\n",
    "    'min_samples_split': np.arange(2,10,2),\n",
    "    'learning_rate': np.arange(.05,1,.25),\n",
    "    'subsample': np.arange(.1,1,.2),\n",
    "}\n",
    "\n",
    "# The below code is property of Gemini. I have had issues before with creating custom scoring functions, so I apologize if anyone reading this notebook does't like that.\n",
    "def custom_mae(y_true, y_pred):\n",
    "    y_true_original = np.exp(y_true)\n",
    "    y_pred_original = np.exp(y_pred)\n",
    "    return mean_absolute_error(y_true_original, y_pred_original)\n",
    "custom_scorer = make_scorer(custom_mae, greater_is_better=False) \n",
    "\n",
    "\n",
    "# The below line of code specifies an initial Gradient Boosting Regressor.\n",
    "gbr = GradientBoostingRegressor(random_state=22)\n",
    "\n",
    "# The following initializes a RandomizedSearchCV model with the Gradient Boosting Regressor, using param_grid for hyperparameters, custom_scorer as the scoring metric, 5-fold cross-validation, and a random state for reproducibility.\n",
    "rscv = RandomizedSearchCV(gbr, param_distributions=param_grid, scoring=custom_scorer, cv=5, return_train_score=True, random_state=22)\n",
    "\n",
    "# Fitting rscv with data transformed by big_pipeline2.\n",
    "rscv.fit(train_2, train_target_log)\n",
    "\n",
    "\n",
    "model = rscv.best_estimator_\n",
    "# Fitting the best estimator with train data transformed by big_pipeline2 and testing said model on test data transformed by big_pipeline2.\n",
    "model.fit(train_2, train_target_log)\n",
    "pred_target_log = model.predict(train_2)\n",
    "pred_target = np.exp(pred_target_log)\n",
    "mae = np.round(mean_absolute_error(train_target, pred_target))\n",
    "print('MAE on training data with second pipeline:', mae)\n",
    "pred_target_log = model.predict(test_2)\n",
    "pred_target = np.exp(pred_target_log)\n",
    "mae = np.round(mean_absolute_error(test_target, pred_target))\n",
    "print('MAE on testing data with second pipeline', mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, the adjustments we made did lead to the model generalizing better than previously, but the testing MAE is worse than it was earlier, so I don't think the adjustments made are the rescue to this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it has been easier to just feed the parameters into a function, it may be not working as I hoped unfortunately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeing if removing less important features will produce a better regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Square feet</td>\n",
       "      <td>0.457026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lot size</td>\n",
       "      <td>0.104781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Year built</td>\n",
       "      <td>0.098736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Baths</td>\n",
       "      <td>0.085818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hoa/month</td>\n",
       "      <td>0.075080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Zip or postal code_66208.0</td>\n",
       "      <td>0.037357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beds</td>\n",
       "      <td>0.016583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>City_Prairie Village</td>\n",
       "      <td>0.014690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>City_Mission Hills</td>\n",
       "      <td>0.013400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Zip or postal code_66061.0</td>\n",
       "      <td>0.010628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Zip or postal code_66205.0</td>\n",
       "      <td>0.010300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Zip or postal code_66212.0</td>\n",
       "      <td>0.006433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>City_Olathe</td>\n",
       "      <td>0.005811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Zip or postal code_66221.0</td>\n",
       "      <td>0.005384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Zip or postal code_66224.0</td>\n",
       "      <td>0.005019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Zip or postal code_66207.0</td>\n",
       "      <td>0.005009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>City_Overland Park</td>\n",
       "      <td>0.004917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Zip or postal code_66226.0</td>\n",
       "      <td>0.003949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Zip or postal code_66209.0</td>\n",
       "      <td>0.003824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>City_Leawood</td>\n",
       "      <td>0.003406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>City_Westwood</td>\n",
       "      <td>0.003180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Zip or postal code_66223.0</td>\n",
       "      <td>0.002596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>City_Merriam</td>\n",
       "      <td>0.002564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Zip or postal code_66062.0</td>\n",
       "      <td>0.002415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>City_Shawnee</td>\n",
       "      <td>0.002335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Zip or postal code_66213.0</td>\n",
       "      <td>0.002232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Zip or postal code_66203.0</td>\n",
       "      <td>0.002204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>City_Roeland Park</td>\n",
       "      <td>0.002054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Zip or postal code_66204.0</td>\n",
       "      <td>0.001813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Zip or postal code_66210.0</td>\n",
       "      <td>0.001779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Zip or postal code_66216.0</td>\n",
       "      <td>0.001674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Zip or postal code_66030.0</td>\n",
       "      <td>0.001381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>City_Gardner</td>\n",
       "      <td>0.001236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>City_Mission</td>\n",
       "      <td>0.001147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Zip or postal code_66202.0</td>\n",
       "      <td>0.001118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>City_Lenexa</td>\n",
       "      <td>0.000735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Zip or postal code_66085.0</td>\n",
       "      <td>0.000450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Zip or postal code_66215.0</td>\n",
       "      <td>0.000283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Zip or postal code_66219.0</td>\n",
       "      <td>0.000223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>City_Westwood Hills</td>\n",
       "      <td>0.000146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Zip or postal code_66220.0</td>\n",
       "      <td>0.000115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Zip or postal code_66214.0</td>\n",
       "      <td>0.000114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Zip or postal code_66211.0</td>\n",
       "      <td>0.000053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>City_Spring Hill</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Zip or postal code_66218.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Zip or postal code_66013.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Zip or postal code_66206.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Zip or postal code_66225.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Zip or postal code_66083.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      features  importance\n",
       "2                  Square feet    0.457026\n",
       "3                     Lot size    0.104781\n",
       "4                   Year built    0.098736\n",
       "1                        Baths    0.085818\n",
       "5                    Hoa/month    0.075080\n",
       "32  Zip or postal code_66208.0    0.037357\n",
       "0                         Beds    0.016583\n",
       "14        City_Prairie Village    0.014690\n",
       "11          City_Mission Hills    0.013400\n",
       "22  Zip or postal code_66061.0    0.010628\n",
       "29  Zip or postal code_66205.0    0.010300\n",
       "36  Zip or postal code_66212.0    0.006433\n",
       "12                 City_Olathe    0.005811\n",
       "44  Zip or postal code_66221.0    0.005384\n",
       "46  Zip or postal code_66224.0    0.005019\n",
       "31  Zip or postal code_66207.0    0.005009\n",
       "13          City_Overland Park    0.004917\n",
       "48  Zip or postal code_66226.0    0.003949\n",
       "33  Zip or postal code_66209.0    0.003824\n",
       "7                 City_Leawood    0.003406\n",
       "18               City_Westwood    0.003180\n",
       "45  Zip or postal code_66223.0    0.002596\n",
       "9                 City_Merriam    0.002564\n",
       "23  Zip or postal code_66062.0    0.002415\n",
       "16                City_Shawnee    0.002335\n",
       "37  Zip or postal code_66213.0    0.002232\n",
       "27  Zip or postal code_66203.0    0.002204\n",
       "15           City_Roeland Park    0.002054\n",
       "28  Zip or postal code_66204.0    0.001813\n",
       "34  Zip or postal code_66210.0    0.001779\n",
       "40  Zip or postal code_66216.0    0.001674\n",
       "21  Zip or postal code_66030.0    0.001381\n",
       "6                 City_Gardner    0.001236\n",
       "10                City_Mission    0.001147\n",
       "26  Zip or postal code_66202.0    0.001118\n",
       "8                  City_Lenexa    0.000735\n",
       "25  Zip or postal code_66085.0    0.000450\n",
       "39  Zip or postal code_66215.0    0.000283\n",
       "42  Zip or postal code_66219.0    0.000223\n",
       "19         City_Westwood Hills    0.000146\n",
       "43  Zip or postal code_66220.0    0.000115\n",
       "38  Zip or postal code_66214.0    0.000114\n",
       "35  Zip or postal code_66211.0    0.000053\n",
       "17            City_Spring Hill    0.000002\n",
       "41  Zip or postal code_66218.0    0.000000\n",
       "20  Zip or postal code_66013.0    0.000000\n",
       "30  Zip or postal code_66206.0    0.000000\n",
       "47  Zip or postal code_66225.0    0.000000\n",
       "24  Zip or postal code_66083.0    0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extracting the feature names from the one-hot encoded columns, combining them with the numerical features, and creating data frames with the data transformed by big_pipeline2.\n",
    "cat_encoder = big_pipeline2.named_transformers_['cat']\n",
    "cat_one_hot_attribs = cat_encoder.get_feature_names_out(cat_feat)\n",
    "attributes = num_feat + list(cat_one_hot_attribs)\n",
    "train_2 = pd.DataFrame(train_2, columns=attributes)\n",
    "test_2 = pd.DataFrame(test_2, columns=attributes)\n",
    "\n",
    "# Creating data frame with feature name and feature importance.\n",
    "f_impt = pd.DataFrame()\n",
    "f_impt['features'] = attributes\n",
    "f_impt['importance'] = model.feature_importances_\n",
    "\n",
    "# Displaying the feature importance data frame.\n",
    "display(f_impt.sort_values(by='importance', ascending=False))\n",
    "\n",
    "# Filtering to only include features with importance greater than 0 and creating a list of those features.\n",
    "imp = f_impt[f_impt['importance'] > 0]\n",
    "impt_feat = imp['features'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new train and test data frames with only the importance features.\n",
    "train_3 = train_2[impt_feat]\n",
    "test_3 = test_2[impt_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below model was trained and tested with all features\n",
      "MAE: 58724.0\n",
      "MAE: 73305.0\n",
      "\n",
      "Below model was trained and tested with only features with importance greater than 0\n",
      "MAE: 60275.0\n",
      "MAE: 74701.0\n"
     ]
    }
   ],
   "source": [
    "# Fitting model with train data with all features and testing said model on test data will all features.\n",
    "model.fit(train_2, train_target_log)\n",
    "pred_target_log = model.predict(train_2)\n",
    "pred_target = np.exp(pred_target_log)\n",
    "mae = np.round(mean_absolute_error(train_target, pred_target))\n",
    "print('Below model was trained and tested with all features')\n",
    "print('MAE:', mae)\n",
    "pred_target_log = model.predict(test_2)\n",
    "pred_target = np.exp(pred_target_log)\n",
    "mae = np.round(mean_absolute_error(test_target, pred_target))\n",
    "print('MAE:', mae)\n",
    "\n",
    "\n",
    "# Fitting model with train data with only the important features and testing said model on test data with only the important features.\n",
    "model.fit(train_3, train_target_log)\n",
    "pred_target_log = model.predict(train_3)\n",
    "pred_target = np.exp(pred_target_log)\n",
    "mae = np.round(mean_absolute_error(train_target, pred_target))\n",
    "print()\n",
    "print('Below model was trained and tested with only features with importance greater than 0')\n",
    "print('MAE:', mae)\n",
    "pred_target_log = model.predict(test_3)\n",
    "pred_target = np.exp(pred_target_log)\n",
    "mae = np.round(mean_absolute_error(test_target, pred_target))\n",
    "print('MAE:', mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the results were not promising as the MAE for both the train and test data increased. While it would be logical to remove features that have no importance, it wasn't in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I first started this project, the main goal was to expand a project I completed in school, and utilize more complex machine learning regressors to create a semi-accurate model to predict the selling prices for houses in Johnson County Kansas. \n",
    "\n",
    "Fast forward to the end of this project, I have become discouraged at the results. After initially exploring through six different model possibilities, I honed in on three possible models to further explore: support vector machine regressor, random forest regressor, and gradient boosting regressor. \n",
    "\n",
    "In the support vector machine notebook, I enjoyed learning more about the more impactful parameters that played a role, along with making the sure the notebook was well organized and documented properly. When I got to the Random Forest notebook, I realized that this feeling didn't occur anymore, and the quality of the notebook decreased. The main thing I told myself during this moment was that this wasn't the right model, and that the gradient boosting regressor would make everything okay. \n",
    "\n",
    "During the gradient boosting regressor notebook, I soon realized that it seemed that the lowest MAE any model could have on the test data was around 65,000-70,000. While not the worst MAE, this would indicate that any model would be off on average by $65000 in determining the selling prices for houses in Johnson County. Additionally, whenever I tinkered with the parameters, it seemed that only the training data score would decrease. When attempts to alleviate over fitting were successful, it came with a cost of the test data MAE growing worse.\n",
    "\n",
    "During this notebook, I once again somewhat attempted to utilize different parameter options for the Randomized Search CV function in hopes of helping the model generalize better to unseen data, but the model only seemed to grow strong in terms of the training data. While lowering the possible parameter combination seemed to help the model generalize better, it once again came with the cost of the model's test data MAE increasing. \n",
    "\n",
    "The last hurdle for this project was attempting to utilize only the features that were important. Unfortunately, this didn't prove a drastic change. \n",
    "\n",
    "While I have no doubt there is more I could do to tinker with this project, the fact of the matter is that I feel that there are other projects that I could complete better, and potentially excel in.\n",
    "\n",
    "Appreciate everyone who has read into any of this at all.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
